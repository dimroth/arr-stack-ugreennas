# Development Guide

> Generated by Document Project workflow on 2026-02-19

## Prerequisites

| Requirement | Purpose | Installation |
|-------------|---------|-------------|
| Git | Version control | Pre-installed on macOS/Linux |
| SSH client + `sshpass` | NAS access | `brew install hudochenkov/sshpass/sshpass` (macOS) |
| Docker (on NAS) | Runtime | Pre-installed on UGOS; app store on Synology/QNAP |
| Python 3 + PyYAML | YAML validation in pre-commit | `pip3 install pyyaml` (optional, basic checks work without) |
| BATS | Bash testing | Git submodules in `tests/` |

## Environment Setup

### 1. Clone the Repository

```bash
git clone https://github.com/Pharkie/arr-stack-ugreennas.git
cd arr-stack-ugreennas
```

### 2. Initialize Test Submodules

```bash
git submodule update --init --recursive
```

### 3. Install Pre-commit Hooks

```bash
./setup-hooks.sh
```

This symlinks `scripts/pre-commit` to `.git/hooks/pre-commit`.

### 4. Create Private Config

```bash
cp config.local.md.example config.local.md
# Edit config.local.md with your NAS hostname, IP, username
```

### 5. Set Up NAS SSH Access

```bash
# Create password file for sshpass
python3 -c "with open('$HOME/.ssh/.nas_pass', 'w') as f: f.write(r'YOUR_PASSWORD')"
chmod 600 ~/.ssh/.nas_pass

# Test connection
sshpass -f ~/.ssh/.nas_pass ssh YOUR_USER@YOUR_NAS_IP 'echo connected'
```

## Development Workflow

### Making Changes

1. Edit compose files, scripts, or configs locally
2. Run pre-commit checks manually: `./scripts/pre-commit`
3. Commit (pre-commit hooks run automatically)
4. Push to GitHub
5. Pull on NAS and restart affected services

### Deploying Changes

```bash
# After git push from local:
sshpass -f ~/.ssh/.nas_pass ssh user@nas "cd /volume2/docker/arr-stack && git pull"

# For compose changes:
sshpass -f ~/.ssh/.nas_pass ssh user@nas "cd /volume2/docker/arr-stack && docker compose -f docker-compose.arr-stack.yml up -d --force-recreate"

# For Traefik routing changes only:
sshpass -f ~/.ssh/.nas_pass ssh user@nas "docker restart traefik"
```

### File Types and Their Deploy Actions

| File Changed | Deploy Action |
|-------------|--------------|
| `docker-compose.*.yml` | `docker compose up -d --force-recreate` |
| `traefik/dynamic/*.yml` | `docker restart traefik` (auto-detected by Traefik) |
| `pihole/02-local-dns.conf` | `docker exec pihole pihole reloaddns` |
| `recyclarr/recyclarr.yml` | `docker restart recyclarr` |
| `scripts/*.sh` | `git pull` on NAS (scripts run from repo path) |
| `docs/*.md` | No deploy needed (documentation only) |

## Testing

### Run BATS Tests

```bash
./tests/run-tests.sh
```

### Run Pre-commit Checks Manually

```bash
./scripts/pre-commit
```

### Test Individual Check Modules

```bash
# Source common library first
source scripts/lib/common.sh

# Then source and run individual checks
source scripts/lib/check-secrets.sh
check_secrets

source scripts/lib/check-conflicts.sh
check_conflicts
```

## Configuration File Patterns

### The `.example` Pattern

Files requiring domain/credential customization use the template pattern:

```
traefik/traefik.yml.example  →  traefik/traefik.yml (gitignored)
.env.example                 →  .env (gitignored)
config.local.md.example      →  config.local.md (gitignored)
```

**Rules:**
- `.example` files are tracked and contain placeholders (`yourdomain.com`)
- Actual files are gitignored and contain real values
- `git pull` updates only templates, never user configs
- New features require manual merge from `.example` to actual file

### Environment Variables (.env)

Organized by setup level in `.env.example`:
1. **Core** (required): `MEDIA_ROOT`, `TZ`, `PUID/PGID`, VPN credentials
2. **+ Local DNS**: `NAS_IP`, `PIHOLE_UI_PASS`, macvlan IPs/MACs
3. **+ Remote Access**: `DOMAIN`, `TRAEFIK_DASHBOARD_AUTH`
4. **+ Tailscale**: `TS_AUTHKEY`, `TAILSCALE_LAN_IP`
5. **Utilities**: qbit-scheduler, Beszel, DIUN configs
6. **TeslaMate**: encryption key, database password
7. **Immich**: version, database password

**Gotcha:** Bcrypt hashes in `.env` must be single-quoted (contain `$`):
```bash
TRAEFIK_DASHBOARD_AUTH='admin:$2y$05$abc...'
```

## Adding a New Service

1. **Add to compose file** with static IP:
   ```yaml
   myservice:
     image: myimage:tag
     container_name: myservice
     networks:
       arr-stack:
         ipv4_address: 172.20.0.XX  # Pick unused IP
     restart: always
     logging: *default-logging
     healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:PORT/"]
       interval: 1m
       timeout: 10s
       retries: 3
   ```

2. **Add `.lan` domain** in `pihole/02-local-dns.conf`:
   ```
   address=/myservice.lan/TRAEFIK_LAN_IP
   ```

3. **Add Traefik route** in `traefik/dynamic/` (appropriate file):
   ```yaml
   http:
     routers:
       myservice-lan:
         rule: "Host(`myservice.lan`)"
         entryPoints: [web]
         service: myservice-lan
     services:
       myservice-lan:
         loadBalancer:
           servers:
             - url: "http://172.20.0.XX:PORT"
   ```

4. **Update `.env.example`** if new env vars are needed

5. **Add Uptime Kuma monitor** (if web UI exists):
   ```bash
   docker exec uptime-kuma sqlite3 /app/data/kuma.db \
     "INSERT INTO monitor (name, type, url, interval, accepted_statuscodes_json, active, maxretries, user_id) \
      VALUES ('MyService', 'http', 'http://myservice.lan/', 60, '[\"200-299\"]', 1, 3, 1);"
   docker restart uptime-kuma
   ```

6. **Add Uptime Kuma `extra_hosts`** entry in `docker-compose.utilities.yml`:
   ```yaml
   extra_hosts:
     - "myservice.lan:172.20.0.2"  # Route to Traefik bridge IP
   ```

## Adding a VPN-Protected Service

Same as above, but with network namespace sharing:

```yaml
myservice:
  image: myimage:tag
  container_name: myservice
  network_mode: "service:gluetun"  # Share Gluetun's network
  depends_on:
    gluetun:
      condition: service_healthy
      restart: true
  labels:
    - deunhealth.restart.on.unhealthy=true  # Auto-recovery
```

Port mappings go on the `gluetun` service, not on the new service.

## Code Conventions

### Compose Files

- Every service has `restart: always`
- Every service has a `healthcheck`
- Every service uses `logging: *default-logging`
- Static IPs for all services on `arr-stack` network
- Descriptive comment blocks above each service
- Services with web UIs have `ports:` for direct `NAS_IP:PORT` access

### Scripts

- `set -e` for error handling (except backup script)
- Color output with terminal detection
- Source `scripts/lib/common.sh` for shared functions
- SSH operations use `sshpass -f ~/.ssh/.nas_pass`
- Graceful handling of SSH failures with `|| true`

### Documentation

- Public docs use placeholders: `yourdomain.com`, `NAS_IP`, `your_password_here`
- Private config in gitignored `config.local.md`
- No real credentials, hostnames, or IPs in tracked files
