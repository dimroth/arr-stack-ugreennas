
#####################################################################
# ⚠️  NEVER use 'docker compose down' - kills Pi-hole DNS!        #
# ⚠️  You WILL lose internet before you can run 'up -d'           #
#                                                                   #
# ✅ Safe restart: docker compose -f docker-compose.arr-stack.yml up -d --force-recreate
# ✅ Or use:       ./scripts/restart-stack.sh                      #
#####################################################################

#########################
# Centralized Volumes   #
#########################
volumes:
  gluetun-config:
  qbittorrent-config:
  sabnzbd-config:
  sonarr-config:
  prowlarr-config:
  radarr-config:
  jellyfin-config:
  jellyfin-cache:
  seerr-config:
  bazarr-config:
  pihole-etc-pihole:
  pihole-etc-dnsmasq:
  tailscale-state:
  teslamate-db:
  teslamate-grafana-data:
  teslamate-mosquitto-conf:
  teslamate-mosquitto-data:
  immich-postgres:
  immich-redis:
  immich-ml-cache:
  recyclarr-config:

#########################
# Networks              #
#########################
networks:
  vpn-net:
    driver: bridge
    ipam:
      config:
        - subnet: 10.8.1.0/24
  arr-stack:
    external: true
  traefik-lan:
    external: true

#########################
# Logging               #
#########################
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

#########################
# Services              #
#########################
services:
  # ═══════════════════════════════════════════════════════════════════════════
  # GLUETUN - VPN gateway. Routes download traffic through your VPN provider.
  # All services below marked "via VPN" share this network for privacy.
  # ═══════════════════════════════════════════════════════════════════════════
  gluetun:
    image: qmcgaw/gluetun:v3.41
    container_name: gluetun
    depends_on:
      pihole:
        condition: service_healthy  # Wait for Pi-hole DNS before VPN connects
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    volumes:
      - gluetun-config:/gluetun
    environment:
      # VPN Configuration - see .env.example for provider-specific setup
      - VPN_SERVICE_PROVIDER=${VPN_SERVICE_PROVIDER}
      - VPN_TYPE=${VPN_TYPE}
      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}
      - WIREGUARD_ADDRESSES=${WIREGUARD_ADDRESSES}
      - WIREGUARD_ENDPOINT_IP=${WIREGUARD_ENDPOINT_IP}
      - WIREGUARD_ENDPOINT_PORT=${WIREGUARD_ENDPOINT_PORT}
      - WIREGUARD_PUBLIC_KEY=${WIREGUARD_PUBLIC_KEY}
      - SERVER_COUNTRIES=${VPN_COUNTRIES}
      - TZ=${TZ}
      - DNS_ADDRESS=172.20.0.5
      - FIREWALL_OUTBOUND_SUBNETS=${LAN_SUBNET},172.20.0.0/24,10.8.1.0/24
    ports:
      # Ports for services using network_mode: "service:gluetun"
      # Enables direct local access without Traefik (http://NAS_IP:PORT)
      - "8989:8989"   # Sonarr
      - "7878:7878"   # Radarr
      - "9696:9696"   # Prowlarr
      - "8085:8085"   # qBittorrent
      - "8082:8080"   # SABnzbd
    networks:
      arr-stack:
        ipv4_address: 172.20.0.3
      vpn-net:
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s  # Give VPN time to establish connection

  # ═══════════════════════════════════════════════════════════════════════════
  # QBITTORRENT - Torrent download client. Downloads files through VPN.
  # Access: qbit.lan | qbit.yourdomain.com | NAS_IP:8085
  # ═══════════════════════════════════════════════════════════════════════════
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:5.1.4
    container_name: qbittorrent
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
    environment:
      - PUID=${PUID}
      - PGID=${PGID}
      - TZ=${TZ}
      - WEBUI_PORT=8085
      - DOCKER_MODS=ghcr.io/vuetorrent/vuetorrent-lsio-mod:latest
    volumes:
      - qbittorrent-config:/config
      - ${MEDIA_ROOT}/downloads:/downloads
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8085/ && wget -q --spider --timeout=5 http://www.google.com"]
      interval: 2m
      timeout: 15s
      retries: 2
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # SABNZBD - Usenet download client. Downloads via VPN for extra security.
  # Access: sabnzbd.lan | sabnzbd.yourdomain.com | NAS_IP:8082
  # ═══════════════════════════════════════════════════════════════════════════
  sabnzbd:
    image: lscr.io/linuxserver/sabnzbd:4.5.5
    container_name: sabnzbd
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
    environment:
      - PUID=${PUID}
      - PGID=${PGID}
      - TZ=${TZ}
    volumes:
      - sabnzbd-config:/config
      - ${MEDIA_ROOT}/downloads:/downloads
      - ${MEDIA_ROOT}/downloads/incomplete:/incomplete-downloads
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/ && wget -q --spider --timeout=5 http://www.google.com"]
      interval: 3m
      timeout: 15s
      retries: 2
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # SONARR - TV show monitor. Watches for new episodes and sends to download.
  # Access: sonarr.lan | sonarr.yourdomain.com | NAS_IP:8989
  # ═══════════════════════════════════════════════════════════════════════════
  sonarr:
    image: lscr.io/linuxserver/sonarr:4.0.16
    container_name: sonarr
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
    environment:
      - PUID=0
      - PGID=0
      - TZ=${TZ}
    volumes:
      - sonarr-config:/config
      - ${MEDIA_ROOT}/tv:/tv
      - ${MEDIA_ROOT}/downloads:/downloads
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8989/ && wget -q --spider --timeout=5 http://www.google.com"]
      interval: 2m
      timeout: 15s
      retries: 2
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # PROWLARR - Indexer manager. Finds download sources for Sonarr/Radarr.
  # Access: prowlarr.lan | prowlarr.yourdomain.com | NAS_IP:9696
  # ═══════════════════════════════════════════════════════════════════════════
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:2.3.0
    container_name: prowlarr
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
    environment:
      - PUID=${PUID}
      - PGID=${PGID}
      - TZ=${TZ}
    volumes:
      - prowlarr-config:/config
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9696/ && wget -q --spider --timeout=5 http://www.google.com"]
      interval: 2m
      timeout: 15s
      retries: 2
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # RADARR - Movie monitor. Watches for new movies and sends to download.
  # Access: radarr.lan | radarr.yourdomain.com | NAS_IP:7878
  # ═══════════════════════════════════════════════════════════════════════════
  radarr:
    image: lscr.io/linuxserver/radarr:6.0.4
    container_name: radarr
    network_mode: "service:gluetun"
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
    environment:
      - PUID=0
      - PGID=0
      - TZ=${TZ}
    volumes:
      - radarr-config:/config
      - ${MEDIA_ROOT}/movies:/movies
      - ${MEDIA_ROOT}/downloads:/downloads
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:7878/ && wget -q --spider --timeout=5 http://www.google.com"]
      interval: 2m
      timeout: 15s
      retries: 2
      start_period: 60s

  # ═══════════════════════════════════════════════════════════════════════════
  # JELLYFIN - Media server. Stream your movies and TV shows like Netflix.
  # Access: jellyfin.lan | jellyfin.yourdomain.com | NAS_IP:8096
  # ═══════════════════════════════════════════════════════════════════════════
  jellyfin:
    image: jellyfin/jellyfin:10.11
    container_name: jellyfin
    ports:
      - "8096:8096"     # Web UI
      - "7359:7359/udp" # Client discovery (auto-detect on LAN)
      - "1900:1900/udp" # DLNA
    environment:
      - TZ=${TZ}
    # Hardware transcoding (Intel Quick Sync) - remove these 4 lines if no Intel GPU
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - "${RENDER_GROUP_ID:-0}"  # Find ID: getent group render | cut -d: -f3
    volumes:
      - jellyfin-config:/config
      - jellyfin-cache:/cache
      - ${MEDIA_ROOT}/movies:/media/movies:ro
      - ${MEDIA_ROOT}/tv:/media/tv:ro
    networks:
      arr-stack:
        ipv4_address: 172.20.0.4
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8096/health"]
      interval: 1m
      timeout: 30s
      retries: 2
      start_period: 30s

  # ═══════════════════════════════════════════════════════════════════════════
  # PI-HOLE - DNS + DHCP server. Enables .lan domains, blocks ads, assigns IPs.
  # Access: pihole.lan/admin | PIHOLE_LAN_IP/admin (macvlan IP)
  # For: Home Pro + Anywhere setups
  # ═══════════════════════════════════════════════════════════════════════════
  pihole:
    image: pihole/pihole:2025.11.1
    container_name: pihole
    # No port mappings needed - Pi-hole is accessed directly on its macvlan LAN IP
    # (Docker port mappings break when macvlan is attached due to NAT return routing)
    networks:
      arr-stack:
        ipv4_address: 172.20.0.5
      vpn-net:
        ipv4_address: 10.8.1.200
      # macvlan gives Pi-hole a real LAN IP for serving DHCP
      # DHCP requires broadcast access to the physical network
      traefik-lan:
        ipv4_address: ${PIHOLE_LAN_IP}
        mac_address: ${PIHOLE_LAN_MAC}
    environment:
      - TZ=${TZ}
      - FTLCONF_webserver_api_password=${PIHOLE_UI_PASS}
      - FTLCONF_dns_listeningMode=all
    volumes:
      - pihole-etc-pihole:/etc/pihole
      - pihole-etc-dnsmasq:/etc/dnsmasq.d
      - ./pihole/02-local-dns.conf:/etc/dnsmasq.d/02-local-dns.conf:ro
    cap_add:
      - NET_ADMIN
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "dig", "@127.0.0.1", "-p", "53", "google.com", "+short"]
      interval: 30s
      timeout: 10s
      retries: 2
      start_period: 30s

  # ═══════════════════════════════════════════════════════════════════════════
  # TAILSCALE - Mesh VPN subnet router. Gives remote devices full LAN access.
  # Unlike Cloudflare Tunnel (HTTP only), provides network-level access to all
  # .lan domains and admin UIs. Free for personal use, works behind CGNAT.
  #
  # NOTE: macvlan (traefik-lan) can't reach the Docker host. We add a static
  # route for NAS_IP through the bridge gateway so subnet-routed traffic to
  # the NAS itself (SSH, web UIs) works via Tailscale.
  # ═══════════════════════════════════════════════════════════════════════════
  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    hostname: nas-tailscale
    cap_add:
      - NET_ADMIN
      - NET_RAW
      - SYS_MODULE
    devices:
      - /dev/net/tun:/dev/net/tun
    volumes:
      - tailscale-state:/var/lib/tailscale
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY:-}
      - TS_AUTH_ONCE=true
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_ROUTES=${LAN_SUBNET}
      - TS_EXTRA_ARGS=--advertise-exit-node --accept-routes
      - TS_USERSPACE=false
    command: >-
      sh -c "ip route add ${NAS_IP}/32 via 172.20.0.1 dev eth0 2>/dev/null || true;
      exec /usr/local/bin/containerboot"
    sysctls:
      - net.ipv4.ip_forward=1
      - net.ipv6.conf.all.forwarding=1
    networks:
      arr-stack:
        ipv4_address: 172.20.0.16
      traefik-lan:
        ipv4_address: ${TAILSCALE_LAN_IP}
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "tailscale", "status", "--json"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

  # ═══════════════════════════════════════════════════════════════════════════
  # SEERR - Request portal. Users request movies/shows here.
  # Access: seerr.lan | seerr.yourdomain.com | NAS_IP:5055
  # ═══════════════════════════════════════════════════════════════════════════
  seerr:
    image: ghcr.io/seerr-team/seerr:latest
    init: true
    container_name: seerr
    depends_on:
      gluetun:
        condition: service_healthy  # Needs gluetun to reach Sonarr/Radarr
        restart: true
    ports:
      - "5055:5055"  # Local network access (all interfaces)
    environment:
      - LOG_LEVEL=info
      - TZ=${TZ}
      - PORT=5055
    networks:
      arr-stack:
        ipv4_address: 172.20.0.8
    volumes:
      - seerr-config:/app/config
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:5055/api/v1/status"]
      interval: 2m
      timeout: 10s
      retries: 3

  # ═══════════════════════════════════════════════════════════════════════════
  # BAZARR - Subtitle manager. Automatically downloads subtitles.
  # Access: bazarr.lan | bazarr.yourdomain.com | NAS_IP:6767
  # ═══════════════════════════════════════════════════════════════════════════
  bazarr:
    image: lscr.io/linuxserver/bazarr:latest
    container_name: bazarr
    depends_on:
      gluetun:
        condition: service_healthy  # Needs gluetun to reach Sonarr/Radarr
        restart: true
    ports:
      - "6767:6767"  # Local network access
    environment:
      - PUID=${PUID}
      - PGID=${PGID}
      - TZ=${TZ}
    networks:
      arr-stack:
        ipv4_address: 172.20.0.9
    volumes:
      - bazarr-config:/config
      - ${MEDIA_ROOT}/movies:/movies
      - ${MEDIA_ROOT}/tv:/tv
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6767/"]
      interval: 3m
      timeout: 10s
      retries: 2

  # ═══════════════════════════════════════════════════════════════════════════
  # TESLAMATE - Tesla vehicle data logger. Logs drives, charges, battery health.
  # Access: teslamate.lan | NAS_IP:4000
  # ═══════════════════════════════════════════════════════════════════════════
  teslamate-db:
    image: postgres:17
    container_name: teslamate-db
    environment:
      - POSTGRES_USER=teslamate
      - POSTGRES_PASSWORD=${TESLAMATE_DB_PASS}
      - POSTGRES_DB=teslamate
    volumes:
      - teslamate-db:/var/lib/postgresql/data
    networks:
      arr-stack:
        ipv4_address: 172.20.0.7
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U teslamate"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  mosquitto:
    image: eclipse-mosquitto:2
    container_name: mosquitto
    volumes:
      - teslamate-mosquitto-conf:/mosquitto/config
      - teslamate-mosquitto-data:/mosquitto/data
      - ./mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro
    networks:
      arr-stack:
        ipv4_address: 172.20.0.17
    restart: always
    logging: *default-logging

  teslamate:
    image: teslamate/teslamate:latest
    container_name: teslamate
    depends_on:
      teslamate-db:
        condition: service_healthy
      mosquitto:
        condition: service_started
    ports:
      - "4000:4000"
    environment:
      - ENCRYPTION_KEY=${TESLAMATE_ENCRYPTION_KEY}
      - DATABASE_USER=teslamate
      - DATABASE_PASS=${TESLAMATE_DB_PASS}
      - DATABASE_NAME=teslamate
      - DATABASE_HOST=teslamate-db
      - MQTT_HOST=mosquitto
      - TZ=${TZ}
    networks:
      arr-stack:
        ipv4_address: 172.20.0.6
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "echo > /dev/tcp/localhost/4000"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

  teslamate-grafana:
    image: teslamate/grafana:latest
    container_name: teslamate-grafana
    ports:
      - "3100:3000"
    environment:
      - DATABASE_USER=teslamate
      - DATABASE_PASS=${TESLAMATE_DB_PASS}
      - DATABASE_NAME=teslamate
      - DATABASE_HOST=teslamate-db
    networks:
      arr-stack:
        ipv4_address: 172.20.0.11
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

  # ═══════════════════════════════════════════════════════════════════════════
  # FLARESOLVERR - Cloudflare bypass. Helps Prowlarr access protected sites.
  # No web UI. Used internally by Prowlarr at 172.20.0.10:8191
  # ═══════════════════════════════════════════════════════════════════════════
  flaresolverr:
    image: ghcr.io/flaresolverr/flaresolverr:v3.4.6
    container_name: flaresolverr
    environment:
      - LOG_LEVEL=info
      - TZ=${TZ}
    networks:
      arr-stack:
        ipv4_address: 172.20.0.10
    labels:
      - deunhealth.restart.on.unhealthy=true
    restart: always
    logging: *default-logging
    healthcheck:
      # Actually test Chrome by making a solve request (catches Chrome crashes)
      test: ["CMD", "curl", "-sf", "-X", "POST", "-H", "Content-Type: application/json",
             "-d", "{\"cmd\":\"request.get\",\"url\":\"http://localhost:8191/\",\"maxTimeout\":30000}",
             "http://localhost:8191/v1"]
      interval: 10m
      timeout: 60s
      retries: 2
      start_period: 2m

  # ═══════════════════════════════════════════════════════════════════════════
  # IMMICH - Self-hosted photo/video management. Smart search, facial
  # recognition, mobile backup. Access: immich.lan | immich.bibous.net | NAS_IP:2283
  # ═══════════════════════════════════════════════════════════════════════════
  immich-postgres:
    image: ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0
    container_name: immich-postgres
    environment:
      - POSTGRES_PASSWORD=${IMMICH_DB_PASSWORD}
      - POSTGRES_USER=postgres
      - POSTGRES_DB=immich
      - POSTGRES_INITDB_ARGS=--data-checksums
    volumes:
      - immich-postgres:/var/lib/postgresql/data
    networks:
      arr-stack:
        ipv4_address: 172.20.0.20
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d immich -U postgres"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  immich-redis:
    image: docker.io/valkey/valkey:9-alpine
    container_name: immich-redis
    networks:
      arr-stack:
        ipv4_address: 172.20.0.21
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 30s
      timeout: 5s
      retries: 3

  immich-server:
    image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release}
    container_name: immich-server
    depends_on:
      immich-postgres:
        condition: service_healthy
      immich-redis:
        condition: service_healthy
    ports:
      - "2283:2283"
    environment:
      - DB_HOSTNAME=immich-postgres
      - DB_USERNAME=postgres
      - DB_PASSWORD=${IMMICH_DB_PASSWORD}
      - DB_DATABASE_NAME=immich
      - REDIS_HOSTNAME=immich-redis
      - TZ=${TZ}
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - "${RENDER_GROUP_ID:-0}"
    volumes:
      - /volume1/immich/upload:/usr/src/app/upload
      - /etc/localtime:/etc/localtime:ro
    networks:
      arr-stack:
        ipv4_address: 172.20.0.18
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:2283/api/server/ping"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 30s

  immich-machine-learning:
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}-openvino
    container_name: immich-machine-learning
    environment:
      - TZ=${TZ}
    devices:
      - /dev/dri:/dev/dri
    group_add:
      - "${RENDER_GROUP_ID:-0}"
    volumes:
      - immich-ml-cache:/cache
    networks:
      arr-stack:
        ipv4_address: 172.20.0.19
    restart: always
    logging: *default-logging
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:3003/ping')"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 2m

  # ═══════════════════════════════════════════════════════════════════════════
  # RECYCLARR - TRaSH Guide sync. Applies recommended quality profiles and
  # custom formats to Sonarr/Radarr on a daily schedule. No web UI.
  # ═══════════════════════════════════════════════════════════════════════════
  recyclarr:
    image: ghcr.io/recyclarr/recyclarr
    container_name: recyclarr
    depends_on:
      gluetun:
        condition: service_healthy
        restart: true
    user: ${PUID}:${PGID}
    environment:
      - TZ=${TZ}
      - CRON_SCHEDULE=${RECYCLARR_CRON:-@daily}
    volumes:
      - recyclarr-config:/config
      - ./recyclarr/recyclarr.yml:/config/recyclarr.yml:ro
      - ./recyclarr/secrets.yml:/config/secrets.yml:ro
    networks:
      arr-stack:
        ipv4_address: 172.20.0.22
    restart: always
    logging: *default-logging
